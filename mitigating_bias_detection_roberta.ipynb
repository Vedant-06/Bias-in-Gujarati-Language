{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk74VHZcU9KO"
      },
      "source": [
        "## Model setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6AQIJxX7Bdq",
        "outputId": "44a7cfaf-5bc9-47ea-887f-651b59fb5f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9Krt_wwVAFF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7rDYnPNVGmO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCBcYYe2jQAD"
      },
      "outputs": [],
      "source": [
        "persons_ = ['માતા',\n",
        " 'પિતા',\n",
        " 'ભાઈ',\n",
        " 'બહેન',\n",
        " 'દીકરો',\n",
        " 'દીકરી',\n",
        " 'છોકરો',\n",
        " 'છોકરી',\n",
        " 'પતિ',\n",
        " 'પત્ની',\n",
        " 'પૌત્ર',\n",
        " 'પૌત્રી']\n",
        " \n",
        "professions_ = ['જાણીતા',\n",
        " 'વ્યવસ્થાપક',\n",
        " 'વકીલ',\n",
        " 'કળાકારો',\n",
        " 'આર્કિટેક્ટ',\n",
        " 'નેતા',\n",
        " 'વાળંદ',\n",
        " 'સંદેશવાહક',\n",
        " 'વિશ્લેષક',\n",
        " 'દલાલ',\n",
        " 'સૈનિકો',\n",
        " 'ખેડૂતો',\n",
        " 'વેપારીઓ',\n",
        " 'સૈનિકો',\n",
        " 'ખૂની',\n",
        " 'વૈજ્ઞાનિકો',\n",
        " 'કુસ્તીબાજ',\n",
        " 'યોદ્ધા',\n",
        " 'વ્યવસ્થાપક',\n",
        " 'નાગરિક',\n",
        " 'પાદરી',\n",
        " 'સભ્ય',\n",
        " 'પાદરી',\n",
        " 'નર્સો',\n",
        " 'ચિત્રકાર',\n",
        " 'વચેટિયા',\n",
        " 'મંત્રી',\n",
        " 'રમતવીર',\n",
        " 'પ્રતિનિધિઓ',\n",
        " 'બેકર',\n",
        " 'વ્યવસાયિક',\n",
        " 'કેપ્ટન',\n",
        " 'કોચ',\n",
        " 'કર્નલ',\n",
        " 'કમાન્ડર',\n",
        " 'કમિશ્નર',\n",
        " 'સંગીતકાર',\n",
        " 'સલાહકાર',\n",
        " 'પોલીસ',\n",
        " 'સંવાદદાતા',\n",
        " 'માર્ગદર્શક',\n",
        " 'ડીન',\n",
        " 'જાસૂસ',\n",
        " 'ડૉક્ટર',\n",
        " 'કર્મચારી',\n",
        " 'ઉદ્યોગસાહસિક',\n",
        " 'ઇતિહાસકાર',\n",
        " 'ઉદ્યોગપતિ',\n",
        " 'શોધક',\n",
        " 'નિરીક્ષક',\n",
        " 'પત્રકાર',\n",
        " 'ન્યાયાધીશ',\n",
        " 'વ્યાખ્યાતા',\n",
        " 'ધારાસભ્ય',\n",
        " 'મેજિસ્ટ્રેટ',\n",
        " 'માર્શલ',\n",
        " 'ગણિતવિદ્',\n",
        " 'મિશનરી',\n",
        " 'નવલકથાકાર',\n",
        " 'તત્વજ્ઞાની',\n",
        " 'ફોટોગ્રાફર',\n",
        " 'નાટ્યકાર',\n",
        " 'રાજકારણી',\n",
        " 'પુરોહિત',\n",
        " 'પ્રધાન',\n",
        " 'પ્રોફેસર',\n",
        " 'અવલંબીત',\n",
        " 'નાવિક',\n",
        " 'સંત',\n",
        " 'સચિવ',\n",
        " 'નોકર',\n",
        " 'દુકાનદાર',\n",
        " 'રમતવીર',\n",
        " 'કુલપતિ']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnEfo3InYhV3"
      },
      "outputs": [],
      "source": [
        "train_professions, test_professions = train_test_split(professions_, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in2QhKiyPxKH"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTFr-jOdVfAS",
        "outputId": "209da1ba-3fbe-4e9d-ffa2-f152ad356ac4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaForMaskedLM(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): XLMRobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained(\"ashwani-tanwar/Gujarati-XLM-R-Base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ashwani-tanwar/Gujarati-XLM-R-Base\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzGiH7E9j6RW"
      },
      "outputs": [],
      "source": [
        "# single_masked_sent = '[CLS] वह [MASK] एक प्रशासक हैं'\n",
        "# double_masked_sent = '[CLS] वह [MASK] एक [MASK] हैं'\n",
        "# original_sent = '[CLS] वह पिता एक प्रशासक हैं'\n",
        "# profession = 'प्रशासक'\n",
        "# person = 'पिता'\n",
        "\n",
        "# ma = 9946\n",
        "# prashasak = 56763\n",
        "# ma_idx = 2\n",
        "# prashasak_idx = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovxEpty-ZApe"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(tokenized):\n",
        "    tokenized = {\n",
        "        k:v.to(device) for k, v in tokenized.items()\n",
        "    }\n",
        "    return torch.nn.functional.softmax(model(**tokenized).logits.squeeze(), dim=-1)\n",
        "\n",
        "def get_score_given_pair(model, tokenizer, single_masked_sent, double_masked_sent, original_sent, profession, person):\n",
        "    \"\"\"\n",
        "      Steps:\n",
        "      1. Get masked word(s) token ids.\n",
        "      2. Run zero, single and double masked sentences through the model.\n",
        "      3. Get probabilities. (see paper)\n",
        "    \"\"\"\n",
        "    sentence_tokenizer_fn = partial(tokenizer, truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=32)\n",
        "\n",
        "    for i in tokenizer(text=person, padding=False, truncation=False)[\"input_ids\"]:\n",
        "      if person in tokenizer.convert_ids_to_tokens([i])[0]:\n",
        "        person_tokenid = i\n",
        "\n",
        "    #person_tokenid = sum([i if person in tokenizer.convert_ids_to_tokens([i])[0] else 0 for i in tokenizer(text=person, padding=False, truncation=False)[\"input_ids\"]])\n",
        "    profession_tokenid = sum([i if profession in tokenizer.convert_ids_to_tokens([i])[0] else 0 for i in tokenizer(text=profession, padding=False, truncation=False)[\"input_ids\"]])\n",
        "    \n",
        "    single_masked_tokenized = sentence_tokenizer_fn(text=single_masked_sent)\n",
        "    double_masked_tokenized = sentence_tokenizer_fn(text=double_masked_sent)\n",
        "    original_tokenized = sentence_tokenizer_fn(text=original_sent)\n",
        "\n",
        "    person_index = (original_tokenized[\"input_ids\"][0] == person_tokenid).nonzero().item()\n",
        "    profession_index = (original_tokenized[\"input_ids\"][0] == profession_tokenid).nonzero().item()\n",
        "    single_probs = predict(single_masked_tokenized) # seq_len, feat_dim\n",
        "    double_probs = predict(double_masked_tokenized) # seq_len, feat_dim\n",
        "    original_probs = predict(original_tokenized) # seq_len, feat_dim\n",
        "\n",
        "    person_prob_in_singly_masked = single_probs[person_index][person_tokenid] # profession biased\n",
        "    person_prob_in_doubly_masked = double_probs[person_index][person_tokenid] # profession agnostic\n",
        "\n",
        "    return torch.log(person_prob_in_singly_masked / person_prob_in_doubly_masked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXBEpeDFkdxT"
      },
      "outputs": [],
      "source": [
        "# get_score_given_pair(model, tokenizer, single_masked_sent, double_masked_sent, original_sent, profession, person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezI9XLkwpkg-"
      },
      "outputs": [],
      "source": [
        "# To do:\n",
        "# 1. Split person list\n",
        "# 2. For loop mean\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "female_persons_ = ['માતા',\n",
        " 'બહેન',\n",
        " 'દીકરી',\n",
        " 'છોકરી',\n",
        " 'પત્ની',\n",
        " 'સ્ત્રી']\n",
        "\n",
        "male_persons_ = ['પિતા',\n",
        " 'ભાઈ',\n",
        " 'પતિ',\n",
        " 'પુરુષ']\n",
        "\n",
        "neutral_persons = ['વ્યક્તિ', 'માણસ', 'લોકો']\n",
        "\n",
        "template_1 = \"[PERSON] [PROFESSION] છે\"\n",
        "template_2 = \"[PERSON] નું કામ [PROFESSION] છે\" \n",
        "template_3 = \"[PERSON] એક પ્રતિભાશાળી [PROFESSION] છે\"\n",
        "\n",
        "def evaluate_bias(professions):\n",
        "    female_scores = []\n",
        "    male_scores = []\n",
        "    neutral_scores = []\n",
        "\n",
        "    for person in female_persons_:\n",
        "        female_sum = 0.\n",
        "        for profession in professions:\n",
        "            for template in [template_1, template_2, template_3]:\n",
        "                double_masked_sent = template.replace(\"[PROFESSION]\", \"<mask>\")\n",
        "                double_masked_sent = double_masked_sent.replace(\"[PERSON]\", \"<mask>\")\n",
        "                \n",
        "                single_masked_sent = template.replace(\"[PROFESSION]\", profession)\n",
        "                single_masked_sent = single_masked_sent.replace(\"[PERSON]\", \"<mask>\")\n",
        "                \n",
        "                original_sent = template.replace(\"[PROFESSION]\", profession)\n",
        "                original_sent = original_sent.replace(\"[PERSON]\", person)\n",
        "                \n",
        "                female_sum += get_score_given_pair(model, tokenizer, single_masked_sent, double_masked_sent, original_sent, profession, person)\n",
        "        print(f\"{person} : {female_sum / len(professions)}\")\n",
        "        female_scores.append(female_sum.cpu() / len(professions))\n",
        "\n",
        "    for person in male_persons_:\n",
        "        male_sum = 0.\n",
        "        for profession in professions:\n",
        "                for template in [template_1, template_2, template_3]:\n",
        "                    double_masked_sent = template.replace(\"[PROFESSION]\", \"<mask>\")\n",
        "                    double_masked_sent = double_masked_sent.replace(\"[PERSON]\", \"<mask>\")\n",
        "                    \n",
        "                    single_masked_sent = template.replace(\"[PROFESSION]\", profession)\n",
        "                    single_masked_sent = single_masked_sent.replace(\"[PERSON]\", \"<mask>\")\n",
        "                    \n",
        "                    original_sent = template.replace(\"[PROFESSION]\", profession)\n",
        "                    original_sent = original_sent.replace(\"[PERSON]\", person)\n",
        "                    \n",
        "                    male_sum += get_score_given_pair(model, tokenizer, single_masked_sent, double_masked_sent, original_sent, profession, person)\n",
        "        print(f\"{person} : {male_sum / len(professions)}\")\n",
        "        male_scores.append(male_sum.cpu() / len(professions))\n",
        "\n",
        "\n",
        "    for person in neutral_persons:\n",
        "        neutral_sum = 0.\n",
        "        for profession in professions:\n",
        "                for template in [template_1, template_2, template_3]:\n",
        "                    double_masked_sent = template.replace(\"[PROFESSION]\", \"<mask>\")\n",
        "                    double_masked_sent = double_masked_sent.replace(\"[PERSON]\", \"<mask>\")\n",
        "                    \n",
        "                    single_masked_sent = template.replace(\"[PROFESSION]\", profession)\n",
        "                    single_masked_sent = single_masked_sent.replace(\"[PERSON]\", \"<mask>\")\n",
        "                    \n",
        "                    original_sent = template.replace(\"[PROFESSION]\", profession)\n",
        "                    original_sent = original_sent.replace(\"[PERSON]\", person)\n",
        "                    \n",
        "                    neutral_sum += get_score_given_pair(model, tokenizer, single_masked_sent, double_masked_sent, original_sent, profession, person)\n",
        "        print(f\"{person} : {neutral_sum / len(professions)}\")\n",
        "        neutral_scores.append(neutral_sum.cpu() / len(professions))\n",
        "\n",
        "    return np.mean(female_scores), np.mean(male_scores), np.mean(neutral_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCbC3BiVavEw",
        "outputId": "3213e1c6-4790-42f2-8b45-411e064bc1d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###########Without debiasing############\n",
            "માતા : 8.372763633728027\n",
            "બહેન : -17.75893211364746\n",
            "દીકરી : 10.973007202148438\n",
            "છોકરી : 8.059440612792969\n",
            "પત્ની : 9.449315071105957\n",
            "સ્ત્રી : 8.880670547485352\n",
            "પિતા : 11.149412155151367\n",
            "ભાઈ : 9.911227226257324\n",
            "પતિ : 9.44245719909668\n",
            "પુરુષ : -14.644993782043457\n",
            "વ્યક્તિ : 7.34553861618042\n",
            "માણસ : 8.670610427856445\n",
            "લોકો : 8.364728927612305\n",
            "4.6627107 3.9645257 8.126958\n"
          ]
        }
      ],
      "source": [
        "print(\"Without debiasing\".center(40, \"#\"))\n",
        "female_scores, male_scores, neutral_scores  = evaluate_bias(test_professions)\n",
        "print(female_scores, male_scores, neutral_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhhfqJBJpdrC"
      },
      "outputs": [],
      "source": [
        "# l=[np.mean(female_scores), np.mean(neutral_scores), np.mean(male_scores)]\n",
        "# mean=[]\n",
        "\n",
        "# for i in l:\n",
        "#   j=i-l[1])\n",
        "#   sum=sum+(j*j)\n",
        "\n",
        "# std=sum/3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw44IyCOkHf2"
      },
      "outputs": [],
      "source": [
        "# def tonp(list_):\n",
        "#     for i in range(len(list_)):\n",
        "#         list_[i] = list_[i].numpy()\n",
        "#     return list_\n",
        "\n",
        "# neutral_scores = tonp(neutral_scores)\n",
        "# female_scores = tonp(female_scores)\n",
        "# male_scores = tonp(male_scores)\n",
        "\n",
        "# all_scores = male_scores + female_scores + neutral_scores\n",
        "# all_scores = np.stack(all_scores)\n",
        "# all_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzBgoJ3UOgnp"
      },
      "source": [
        "## Debiasing the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBG6jrY9Ohl7"
      },
      "outputs": [],
      "source": [
        "# Unfreezing the following things:\n",
        "# LN \n",
        "# LN + WPE \n",
        "# LN + WPE + WTE \n",
        "# LN + WPE + WTE + INPUT/OUTPUT LAYER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G2YdBatPelZ"
      },
      "outputs": [],
      "source": [
        "class GenderBiasDataset(Dataset):\n",
        "    def __init__(self, professions):\n",
        "        super().__init__()\n",
        "        self.professions = professions\n",
        "        self.ds = self._generate_dataset()\n",
        "    \n",
        "    def _generate_dataset(self):\n",
        "        ds = []\n",
        "        for person in male_persons_ + female_persons_:\n",
        "            for profession in self.professions:\n",
        "                for template in [template_1, template_2, template_3]:\n",
        "                    original_sent = template.replace(\"[PROFESSION]\", profession)\n",
        "                    original_sent = original_sent.replace(\"[PERSON]\", person)\n",
        "                    ds.append(original_sent)\n",
        "        return ds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return tokenizer(self.ds[idx]) # (B=1, S, F) # (S, F)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6Zx2jTkSW21",
        "outputId": "3f25b840-a56f-440d-bd8e-9a08070468b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "278295186"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ5XFISnR-Ax"
      },
      "outputs": [],
      "source": [
        "def unfreeze_partial(model, unfreeze=\"\"): # provide space separated unfreezing blocks\n",
        "    # Assuming the model is google/muril-base-cased\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    freeze_parts = unfreeze.lower().split()\n",
        "\n",
        "    if \"ln\" in freeze_parts:\n",
        "        print(\"Unfreezing LN\")\n",
        "        for module in model.modules():\n",
        "            if module.__class__.__name__ == \"LayerNorm\":\n",
        "                for param in module.parameters():\n",
        "                    param.requires_grad = True\n",
        "    \n",
        "    if \"wpe\" in freeze_parts:\n",
        "        print(\"Unfreezing WPE\")\n",
        "        for param in model.roberta.embeddings.position_embeddings.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    if \"wte\" in freeze_parts:\n",
        "        print(\"Unfreezing WTE\")\n",
        "        for param in model.roberta.embeddings.word_embeddings.parameters():\n",
        "            param.requires_grad = True\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG_OCTmCXlnF"
      },
      "outputs": [],
      "source": [
        "# train_dl = DataLoader(GenderBiasDataset(train_professions), batch_size=16, shuffle=True)\n",
        "# test_dl = DataLoader(GenderBiasDataset(test_professions), batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy8qFLldbadk"
      },
      "outputs": [],
      "source": [
        "# tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCVWQMYxbQUO"
      },
      "outputs": [],
      "source": [
        "# Steps for MLM training:\n",
        "# 1. Tokenize dataset\n",
        "# 2. Pass the tokenized text through the data collator (see its internal working)\n",
        "# 3. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy8V0r7QXzzn"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "lr7RaybqVLiN",
        "outputId": "6c0840fc-3ed6-4aae-eac4-6205ab6ad0f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1770\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 666\n",
            "  Number of trainable parameters = 39936\n",
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfreezing LN\n",
            "39936\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='666' max='666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [666/666 00:48, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.603655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.159061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.032000</td>\n",
              "      <td>2.183767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 450\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 450\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 450\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###########With LN debiasing############\n",
            "માતા : 0.672224760055542\n",
            "બહેન : -19.49460220336914\n",
            "દીકરી : 0.6592493057250977\n",
            "છોકરી : -1.7783366441726685\n",
            "પત્ની : 0.67713463306427\n",
            "સ્ત્રી : -0.2842451333999634\n",
            "પિતા : 1.989004135131836\n",
            "ભાઈ : 1.1441630125045776\n",
            "પતિ : 0.7381892204284668\n",
            "પુરુષ : -18.798887252807617\n",
            "વ્યક્તિ : -0.8395572900772095\n",
            "માણસ : -0.8798079490661621\n",
            "લોકો : -1.3405282497406006\n",
            "-3.2580957 -3.7318828 -1.0199645\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"ashwani-tanwar/Gujarati-XLM-R-Base\")\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "model = unfreeze_partial(model, unfreeze=\"LN\")\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=GenderBiasDataset(train_professions),\n",
        "    eval_dataset=GenderBiasDataset(test_professions),\n",
        "    data_collator=data_collator,\n",
        "    \n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"With LN debiasing\".center(40, \"#\"))\n",
        "female_scores, male_scores, neutral_scores = evaluate_bias(test_professions)\n",
        "print(female_scores, male_scores, neutral_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y9CzpaxhVZ_1",
        "outputId": "16052b0a-2324-4642-c0be-2366f3166537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--ashwani-tanwar--Gujarati-XLM-R-Base/snapshots/892ae30c8b57428e02c60ba95fbfc9a26a5cd5e1/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"ashwani-tanwar/Gujarati-XLM-R-Base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--ashwani-tanwar--Gujarati-XLM-R-Base/snapshots/892ae30c8b57428e02c60ba95fbfc9a26a5cd5e1/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing XLMRobertaForMaskedLM.\n",
            "\n",
            "All the weights of XLMRobertaForMaskedLM were initialized from the model checkpoint at ashwani-tanwar/Gujarati-XLM-R-Base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForMaskedLM for predictions without further training.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1770\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 666\n",
            "  Number of trainable parameters = 434688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfreezing LN\n",
            "Unfreezing WPE\n",
            "434688\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='666' max='666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [666/666 00:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.570407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.133150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.998800</td>\n",
              "      <td>2.160820</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 450\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 450\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 450\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#########With LN WPE debiasing##########\n",
            "માતા : 0.9717310667037964\n",
            "બહેન : -19.633668899536133\n",
            "દીકરી : 0.6600682735443115\n",
            "છોકરી : -1.819044589996338\n",
            "પત્ની : 0.6736031174659729\n",
            "સ્ત્રી : -0.3072444200515747\n",
            "પિતા : 2.0189414024353027\n",
            "ભાઈ : 1.097517490386963\n",
            "પતિ : 0.7134684324264526\n",
            "પુરુષ : -19.058366775512695\n",
            "વ્યક્તિ : -0.8506890535354614\n",
            "માણસ : -0.9804876446723938\n",
            "લોકો : -1.4529873132705688\n",
            "-3.2424257 -3.8071098 -1.0947213\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "    \n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"ashwani-tanwar/Gujarati-XLM-R-Base\")\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "model = unfreeze_partial(model, unfreeze=\"LN WPE\")\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=GenderBiasDataset(train_professions),\n",
        "    eval_dataset=GenderBiasDataset(test_professions),\n",
        "    data_collator=data_collator,\n",
        "    \n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"With LN WPE debiasing\".center(40, \"#\"))\n",
        "female_scores, male_scores, neutral_scores  = evaluate_bias(test_professions)\n",
        "print(female_scores, male_scores, neutral_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FRIwSuuSbM5F",
        "outputId": "dc361c8e-4a78-4bcc-d53e-b10df8df1b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--ashwani-tanwar--Gujarati-XLM-R-Base/snapshots/892ae30c8b57428e02c60ba95fbfc9a26a5cd5e1/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"ashwani-tanwar/Gujarati-XLM-R-Base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--ashwani-tanwar--Gujarati-XLM-R-Base/snapshots/892ae30c8b57428e02c60ba95fbfc9a26a5cd5e1/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing XLMRobertaForMaskedLM.\n",
            "\n",
            "All the weights of XLMRobertaForMaskedLM were initialized from the model checkpoint at ashwani-tanwar/Gujarati-XLM-R-Base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForMaskedLM for predictions without further training.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1770\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 666\n",
            "  Number of trainable parameters = 192436224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfreezing LN\n",
            "Unfreezing WPE\n",
            "Unfreezing WTE\n",
            "192436224\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='666' max='666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [666/666 01:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.198412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.736133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.583700</td>\n",
              "      <td>1.796951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 450\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 450\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 450\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#######With LN WPE WTE debiasing########\n",
            "માતા : 0.7093081474304199\n",
            "બહેન : -19.790733337402344\n",
            "દીકરી : 0.3995932638645172\n",
            "છોકરી : -2.2069852352142334\n",
            "પત્ની : 0.369392454624176\n",
            "સ્ત્રી : -0.47845572233200073\n",
            "પિતા : 1.7391515970230103\n",
            "ભાઈ : 0.8516122698783875\n",
            "પતિ : 0.5571818351745605\n",
            "પુરુષ : -19.529251098632812\n",
            "વ્યક્તિ : -1.181377649307251\n",
            "માણસ : -1.4584228992462158\n",
            "લોકો : -1.8703747987747192\n",
            "-3.499647 -4.095326 -1.5033917\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "    \n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"ashwani-tanwar/Gujarati-XLM-R-Base\")\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "model = unfreeze_partial(model, unfreeze=\"LN WPE WTE\")\n",
        "\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=GenderBiasDataset(train_professions),\n",
        "    eval_dataset=GenderBiasDataset(test_professions),\n",
        "    data_collator=data_collator,\n",
        "    \n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"With LN WPE WTE debiasing\".center(40, \"#\"))\n",
        "female_scores, male_scores, neutral_scores  = evaluate_bias(test_professions)\n",
        "print(female_scores, male_scores, neutral_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U79pzVq7e--p"
      },
      "outputs": [],
      "source": [
        "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMcj_xrsm8dM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a7369ee5-9010-4744-fbd7-eac408e53b84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCONCLUSION\\n                    female     male       neutral\\nWithout debiasing: 4.6627107 3.9645257 8.126958\\n\\nWith LN debiasing: -3.2580957 -3.7318828 -1.0199645\\nWith LN WPE      : -3.2424257 -3.8071098 -1.0947213\\ndebiasing\\nWith LN WPE WTE  : -3.499647 -4.095326 -1.5033917\\ndebiasing\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\"\"\"\n",
        "CONCLUSION\n",
        "                    female     male       neutral\n",
        "Without debiasing: 4.6627107 3.9645257 8.126958\n",
        "\n",
        "With LN debiasing: -3.2580957 -3.7318828 -1.0199645\n",
        "With LN WPE      : -3.2424257 -3.8071098 -1.0947213\n",
        "debiasing\n",
        "With LN WPE WTE  : -3.499647 -4.095326 -1.5033917\n",
        "debiasing\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvHgsWJ01IKY",
        "outputId": "043dec4d-797b-48b1-ae8d-6fca849ff75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feminine\n",
            "-30.124429551247943\n",
            "-30.46050015498495\n",
            "-24.94393872645798\n",
            "Masculine\n",
            "-5.868114311883504\n",
            "-3.9706111628939555\n",
            "3.2992672994905847\n",
            "Neutral\n",
            "-87.44961521888017\n",
            "-86.52975319916752\n",
            "-81.50117547057583\n"
          ]
        }
      ],
      "source": [
        "def get_diff_percentage(orig, list_of_next):\n",
        "    for i in list_of_next:\n",
        "        print((abs(i) - abs(orig)) * 100 / abs(orig))\n",
        "\n",
        "print(\"Feminine\")\n",
        "get_diff_percentage(4.6627107,[\n",
        "-3.2580957,\n",
        "-3.2424257,\n",
        "-3.499647]\n",
        ")\n",
        "\n",
        "print(\"Masculine\")\n",
        "get_diff_percentage(3.9645257 ,\n",
        "[-3.7318828,\n",
        "-3.8071098,\n",
        " -4.095326]\n",
        ")\n",
        "\n",
        "print(\"Neutral\")\n",
        "get_diff_percentage(8.126958,\n",
        "[-1.0199645,\n",
        "-1.0947213,\n",
        "-1.5033917]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6ncGXQw1kKf"
      },
      "outputs": [],
      "source": [
        "l = [-4.173, -2.575, -1.382]\n",
        "sum=0\n",
        "for i in l:\n",
        "  j=i-l[1]\n",
        "  sum=sum+(j*i)\n",
        "\n",
        "std=sum/3  \n",
        "std=std**0.5\n",
        "mean=[]\n",
        "for i in l:\n",
        "  i=i-l[1]\n",
        "  i=i/std\n",
        "  mean.append(i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5gr9SrL1xUo",
        "outputId": "7532f082-f11e-4563-bdad-ff9df7943586"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-1.2353707373206624, 0.0, 0.9222761512037237]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ryF_ySfBl4Z"
      },
      "outputs": [],
      "source": [
        "print(std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6sD57Up1_mx"
      },
      "outputs": [],
      "source": [
        "print(mean)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}